{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "from textstat import flesch_reading_ease\n",
    "import nltk\n",
    "from nltk.util import pairwise \n",
    "from vaderSentiment.vaderSentiment import NEGATE, BOOSTER_DICT\n",
    "import re\n",
    "import nltk.data\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Working directory\n",
    "os.chdir('C:/Users/jesse/OneDrive/Documenten/Thesis/amazon_code/dataframes_done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load The JSON file\n",
    "with open('output.json', 'r') as f:\n",
    "    reviews = json.load(f)\n",
    "\n",
    "# DataFrame\n",
    "reviews_df = pd.DataFrame(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantity\n",
    "result_quantity = []\n",
    "\n",
    "for review in reviews:\n",
    "    text = review['Review_Text']\n",
    "    # Number of words\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    # Number of sentences\n",
    "    sentences = text.split('.')\n",
    "    num_sentences = len(sentences)\n",
    "    # Number of caps\n",
    "    num_caps = sum(1 for c in text if c.isupper())\n",
    "    # Number of punctuation\n",
    "    num_punctuation = sum(text.count(p) for p in string.punctuation)\n",
    "     # Part of speech\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    noun_count = len([word for word, pos in pos_tags if pos in ['NN', 'NNS', 'NNP', 'NNPS']])\n",
    "    verb_count = len([word for word, pos in pos_tags if pos in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']])\n",
    "    adj_count = len([word for word, pos in pos_tags if pos in ['JJ', 'JJR', 'JJS']])\n",
    "    adv_count = len([word for word, pos in pos_tags if pos in ['RB', 'RBR', 'RBS']])\n",
    "\n",
    "\n",
    "    # Linguistic features results\n",
    "    result_quantity.append({\n",
    "        'Number_of_words': num_words,\n",
    "        'Number_of_sentences': num_sentences,\n",
    "        'Number_of_caps': num_caps,\n",
    "        'Number_of_punctuation': num_punctuation,\n",
    "        'Number_of_nouns': noun_count,\n",
    "        'Number_of_verbs': verb_count,\n",
    "        'Number_of_adjectives': adj_count,\n",
    "        'Number_of_adverbs': adv_count\n",
    "    })\n",
    "\n",
    "# Result dataframe\n",
    "df_qua_VADER = pd.DataFrame(result_quantity)\n",
    "\n",
    "# Dataframe to Json file\n",
    "df_qua_VADER.to_json('df_qua_VADER.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complexity\n",
    "\n",
    "# Redundancy function\n",
    "def calculate_redundancy(text):\n",
    "    words = text.split()\n",
    "    \n",
    "    # Sum of Levenshtein distances between all pairs of words\n",
    "    total_distance = sum(editdistance.eval(w1, w2) for i, w1 in enumerate(words) for j, w2 in enumerate(words) if i < j)\n",
    "    \n",
    "    # Average Levenshtein distance\n",
    "    n = len(words)\n",
    "    if n > 1:\n",
    "        average_distance = total_distance / (n * (n - 1) / 2)\n",
    "    else:\n",
    "        average_distance = 0\n",
    "    \n",
    "    # Return the redundancy\n",
    "    return 1 - average_distance / len(max(words, key=len))\n",
    "\n",
    "results_complexity = []\n",
    "\n",
    "for review in reviews:\n",
    "    text = review['Review_Text']\n",
    "    # Number of words\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    # Number of sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    # Average word length\n",
    "    total_word_length = sum(len(word) for word in words)\n",
    "    avg_word_length = total_word_length / num_words\n",
    "    # Average sentence length\n",
    "    total_sentence_length = sum(len(sent) for sent in sentences)\n",
    "    avg_sentence_length = total_sentence_length / num_sentences\n",
    "    # Redundance score\n",
    "    redundancy = calculate_redundancy(text)\n",
    "    # Readability score\n",
    "    readability_score = flesch_reading_ease(text)\n",
    "\n",
    "    # linguistic features results\n",
    "    results_complexity.append({\n",
    "        'Average_word_length': avg_word_length,\n",
    "        'Average_sentence_length': avg_sentence_length,\n",
    "        'Redundancy_score': redundancy,\n",
    "        'Readability_score': readability_score,\n",
    "})\n",
    "\n",
    "# Result dataframe\n",
    "df_com_VADER = pd.DataFrame(results_complexity)\n",
    "\n",
    "# Dataframe to Json file\n",
    "df_com_VADER.to_json('df_com_VADER.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diversity\n",
    "results_diversity = []\n",
    "\n",
    "for review in reviews:\n",
    "    text = review['lemmatized']\n",
    "    words = text\n",
    "    num_words = len(words)\n",
    "    # Lexical diversity\n",
    "    unique_words = set(words)\n",
    "    lexical_diversity = len(unique_words) / num_words\n",
    "\n",
    "    # Linguistic features results\n",
    "    results_diversity.append({\n",
    "        'Lexical_diversity': lexical_diversity\n",
    "    })\n",
    "\n",
    "\n",
    "# Results dataframe\n",
    "df_div_VADER = pd.DataFrame(results_diversity)\n",
    "\n",
    "# Dataframe to Json file\n",
    "df_div_VADER.to_json('df_div_VADER.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion\n",
    "\n",
    "# VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Sentiment score function\n",
    "def get_sentiment_scores(text):\n",
    "    scores = sia.polarity_scores(text)\n",
    "    return scores['compound'], scores['pos'], scores['neg'], scores['neu']\n",
    "\n",
    "# Polarity function\n",
    "def get_polarity_categories(text):\n",
    "    words = text.split()\n",
    "    polarities = {'pos': 0, 'neg': 0, 'neu': 0}\n",
    "    for word in words:\n",
    "        scores = sia.polarity_scores(word)\n",
    "        for key in polarities.keys():\n",
    "            if scores[key] > 0:\n",
    "                polarities[key] += 1\n",
    "    return polarities\n",
    "\n",
    "# Polarity shifters function\n",
    "def count_polarity_shifters(text):\n",
    "    shifters = ['but', 'however', 'although', 'yet', 'nevertheless']\n",
    "    count = 0\n",
    "    for word in text:\n",
    "        if word.lower() in shifters:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Intensity modifiers function\n",
    "intensity_modifier_words = BOOSTER_DICT\n",
    "def count_intensity_modifiers(text):\n",
    "    modifiers = intensity_modifier_words\n",
    "    count = 0\n",
    "    for word in text:\n",
    "        if word.lower() in modifiers:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Negations function\n",
    "negation_words = NEGATE\n",
    "def count_negations(text):\n",
    "    negations = negation_words \n",
    "    count = 0\n",
    "    for word in text:\n",
    "        if word.lower() in negations:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_emoticons(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    scores = sid.polarity_scores(text)\n",
    "    num_emoticons = len(emoticons)\n",
    "    return num_emoticons\n",
    "\n",
    "# Dataframe\n",
    "columns = ['sentiment_score', 'positive_score', 'negative_score', 'neutral_score',            \n",
    "           'positive_words', 'negative_words', 'neutral_words',           \n",
    "           'polarity_shifters', 'intensity_modifiers', 'negations', 'emoticons']\n",
    "df_emo_VADER = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Extract features\n",
    "for index, row in reviews_df.iterrows():\n",
    "    text = row['lemmatized']\n",
    "    text2 = row['Review_Text']\n",
    "    sentiment_score, positive_score, negative_score, neutral_score = get_sentiment_scores(text2)\n",
    "    polarities = get_polarity_categories(text2)\n",
    "    polarity_shifters = count_polarity_shifters(text)\n",
    "    intensity_modifiers = count_intensity_modifiers(text)\n",
    "    negations = count_negations(text)\n",
    "    emoticons = count_emoticons(text2)\n",
    "    row_results = [sentiment_score, positive_score, negative_score, neutral_score, \n",
    "                   polarities['pos'], polarities['neg'], polarities['neu'],\n",
    "                   polarity_shifters, intensity_modifiers, negations, emoticons]\n",
    "    df_emo_VADER.loc[index] = row_results\n",
    "\n",
    "# Dataframe to Json file\n",
    "df_emo_VADER.to_json('df_emo_VADER.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory\n",
    "os.chdir('C:/Users/jesse/OneDrive/Documenten/Thesis/amazon_code/dataframes')\n",
    "\n",
    "# Load dataframes\n",
    "df_qua_VADER = pd.read_json('df_qua_VADER.json')\n",
    "df_com_VADER = pd.read_json('df_com_VADER.json')\n",
    "df_div_VADER = pd.read_json('df_div_VADER.json')\n",
    "df_emo_VADER = pd.read_json('df_emo_VADER.json')\n",
    "\n",
    "# Concatenate dataframes\n",
    "VADER_df = pd.concat([reviews_df[['Label']], df_qua_VADER, df_com_VADER, df_div_VADER, df_emo_VADER], axis=1)\n",
    "\n",
    "# Dataframe to Json file\n",
    "VADER_df.to_json('VADER_df.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the labels\n",
    "VADER_df['Label'] = VADER_df['Label'].replace({'__label1__': 'fake', '__label2__': 'real'})\n",
    "\n",
    "# Group the data by label and calculate statistics\n",
    "statistics_VADER = VADER_df.groupby('Label').describe().transpose()\n",
    "\n",
    "# Print statistics\n",
    "print(statistics_VADER)\n",
    "\n",
    "statistics_VADER.to_excel('statistics_VADER.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Label  Number_of_words  Number_of_sentences  Number_of_caps  \\\n",
      "0      0         0.002822             0.010152        0.001144   \n",
      "1      0         0.020106             0.015228        0.002859   \n",
      "2      0         0.014109             0.030457        0.003431   \n",
      "3      0         0.010582             0.020305        0.004002   \n",
      "4      0         0.019400             0.015228        0.002859   \n",
      "\n",
      "   Number_of_punctuation  Number_of_nouns  Number_of_verbs  \\\n",
      "0               0.005357         0.005908         0.009615   \n",
      "1               0.012500         0.026588         0.025000   \n",
      "2               0.017857         0.013294         0.023077   \n",
      "3               0.008929         0.013294         0.019231   \n",
      "4               0.010714         0.023634         0.019231   \n",
      "\n",
      "   Number_of_adjectives  Number_of_adverbs  Average_word_length  ...  \\\n",
      "0              0.007663           0.009174             0.123009  ...   \n",
      "1              0.038314           0.022936             0.200208  ...   \n",
      "2              0.030651           0.022936             0.092044  ...   \n",
      "3              0.011494           0.018349             0.104161  ...   \n",
      "4              0.022989           0.022936             0.102568  ...   \n",
      "\n",
      "   positive_score  negative_score  neutral_score  positive_words  \\\n",
      "0           0.217           0.000          0.783        0.018519   \n",
      "1           0.137           0.060          0.803        0.018519   \n",
      "2           0.165           0.053          0.782        0.055556   \n",
      "3           0.099           0.000          0.901        0.009259   \n",
      "4           0.116           0.000          0.884        0.027778   \n",
      "\n",
      "   negative_words  neutral_words  polarity_shifters  intensity_modifiers  \\\n",
      "0        0.000000       0.004542           0.000000             0.012987   \n",
      "1        0.014085       0.022296           0.055556             0.012987   \n",
      "2        0.000000       0.014038           0.055556             0.038961   \n",
      "3        0.000000       0.011974           0.000000             0.038961   \n",
      "4        0.000000       0.021057           0.000000             0.012987   \n",
      "\n",
      "   negations  emoticons  \n",
      "0   0.000000        0.0  \n",
      "1   0.040816        0.0  \n",
      "2   0.061224        0.0  \n",
      "3   0.020408        0.0  \n",
      "4   0.000000        0.0  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Select the columns to normalize\n",
    "columns_to_normalize = ['positive_words', 'negative_words', 'neutral_words',\n",
    "                        'polarity_shifters', 'intensity_modifiers', 'negations', 'emoticons',\n",
    "                        'Average_word_length', 'Average_sentence_length', 'Number_of_words', 'Number_of_sentences',\n",
    "                        'Number_of_caps', 'Number_of_punctuation', 'Number_of_nouns',\n",
    "                        'Number_of_verbs', 'Number_of_adjectives', 'Number_of_adverbs']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# Normalize\n",
    "VADER_df[columns_to_normalize] = scaler.fit_transform(VADER_df[columns_to_normalize])\n",
    "\n",
    "# Dataframe to Json file\n",
    "VADER_df.to_json('VADER_df_norm.json', orient='records')\n",
    "print(VADER_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
