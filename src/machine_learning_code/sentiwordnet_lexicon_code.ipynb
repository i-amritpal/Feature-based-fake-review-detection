{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\jesse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from collections import Counter\n",
    "from nltk import pos_tag\n",
    "from sklearn.utils import shuffle\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from textstat import flesch_reading_ease\n",
    "import editdistance\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory\n",
    "os.chdir('C:/Users/jesse/OneDrive/Documenten/Thesis/amazon_code/dataframes_done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reviews from the JSON file\n",
    "with open('output.json', 'r') as f:\n",
    "    reviews = json.load(f)\n",
    "\n",
    "# DataFrame\n",
    "reviews_df = pd.DataFrame(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantity\n",
    "result_quantity = []\n",
    "\n",
    "for review in reviews:\n",
    "    text = review['Review_Text']\n",
    "    # Number of words\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    # Number of sentences\n",
    "    sentences = text.split('.')\n",
    "    num_sentences = len(sentences)\n",
    "    # Number of modifiers\n",
    "    modifiers = ['very', 'extremely', 'quite', 'somewhat', 'really']\n",
    "    num_modifiers = sum(text.lower().count(modifier) for modifier in modifiers)\n",
    "    # Number of caps\n",
    "    num_caps = sum(1 for c in text if c.isupper())\n",
    "    # Number of punctuation\n",
    "    num_punctuation = sum(text.count(p) for p in string.punctuation)\n",
    "    # Part of speech \n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    noun_count = len([word for word, pos in pos_tags if pos in ['NN', 'NNS', 'NNP', 'NNPS']])\n",
    "    verb_count = len([word for word, pos in pos_tags if pos in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']])\n",
    "    adj_count = len([word for word, pos in pos_tags if pos in ['JJ', 'JJR', 'JJS']])\n",
    "    adv_count = len([word for word, pos in pos_tags if pos in ['RB', 'RBR', 'RBS']])\n",
    "\n",
    "    # linguistic features results\n",
    "    result_quantity.append({\n",
    "        'Number_of_words': num_words,\n",
    "        'Number_of_sentences': num_sentences,\n",
    "        'Number_of_modifiers': num_modifiers,\n",
    "        'Number_of_caps': num_caps,\n",
    "        'Number_of_punctuation': num_punctuation,\n",
    "        'Number_of_nouns': noun_count,\n",
    "        'Number_of_verbs': verb_count,\n",
    "        'Number_of_adjectives': adj_count,\n",
    "        'Number_of_adverbs': adv_count\n",
    "    })\n",
    "\n",
    "# Result dataframe\n",
    "df_qua_senti = pd.DataFrame(result_quantity)\n",
    "\n",
    "# Dataframe to Json file\n",
    "df_qua_senti.to_json('df_qua_senti.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complexity\n",
    "\n",
    "# Redundancy function\n",
    "def calculate_redundancy(text):\n",
    "    words = text.split()\n",
    "    \n",
    "    # Sum of Levenshtein distances between all pairs of words\n",
    "    total_distance = sum(editdistance.eval(w1, w2) for i, w1 in enumerate(words) for j, w2 in enumerate(words) if i < j)\n",
    "    \n",
    "    # Average Levenshtein distance\n",
    "    n = len(words)\n",
    "    if n > 1:\n",
    "        average_distance = total_distance / (n * (n - 1) / 2)\n",
    "    else:\n",
    "        average_distance = 0\n",
    "    \n",
    "    # Return the redundancy\n",
    "    return 1 - average_distance / len(max(words, key=len))\n",
    "\n",
    "results_complexity = []\n",
    "\n",
    "for review in reviews:\n",
    "    text = review['Review_Text']\n",
    "    # Number of words\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    # Number of sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    # Average word length\n",
    "    total_word_length = sum(len(word) for word in words)\n",
    "    avg_word_length = total_word_length / num_words\n",
    "    # Average sentence length\n",
    "    total_sentence_length = sum(len(sent) for sent in sentences)\n",
    "    avg_sentence_length = total_sentence_length / num_sentences\n",
    "    # Redundance score\n",
    "    redundancy = calculate_redundancy(text)\n",
    "    # Readability score\n",
    "    readability_score = flesch_reading_ease(text)\n",
    "\n",
    "    # linguistic features results\n",
    "    results_complexity.append({\n",
    "        'Average_word_length': avg_word_length,\n",
    "        'Average_sentence_length': avg_sentence_length,\n",
    "        'Redundancy_score': redundancy,\n",
    "        'Readability_score': readability_score,\n",
    "})\n",
    "\n",
    "# Result dataframe\n",
    "df_com_senti = pd.DataFrame(results_complexity)\n",
    "\n",
    "# Dataframe to Json file\n",
    "df_com_senti.to_json('df_com_senti.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diversity\n",
    "results_diversity = []\n",
    "\n",
    "for review in reviews:\n",
    "    text = review['Review_Text']\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    # Lexical diversity\n",
    "    unique_words = set(words)\n",
    "    lexical_diversity = len(unique_words) / num_words\n",
    "\n",
    "    # Linguistic features results\n",
    "    results_diversity.append({\n",
    "        'Lexical_diversity': lexical_diversity\n",
    "    })\n",
    "\n",
    "# Results dataframe\n",
    "df_div_senti = pd.DataFrame(results_diversity)\n",
    "\n",
    "# Dataframe to Json file\n",
    "df_div_senti.to_json('df_div_senti.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion\n",
    "\n",
    "# Sentiment score function per word\n",
    "def get_sentiment(word):\n",
    "    sentiment = 0.0\n",
    "    synsets = nltk.corpus.wordnet.synsets(word)\n",
    "    if not synsets:\n",
    "        return sentiment\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "    sentiment = swn_synset.pos_score() - swn_synset.neg_score()\n",
    "    return sentiment\n",
    "\n",
    "# Sentiment score function per sentence\n",
    "def get_sentiment_score(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    sentiment_score = sum(get_sentiment(word) for word in words)\n",
    "    return sentiment_score\n",
    "\n",
    "# Intensity score function\n",
    "def get_intensity_score(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    intensity_score = sum(get_sentiment(word) ** 2 for word in words)\n",
    "    return intensity_score\n",
    "\n",
    "# Ambiguity score function\n",
    "def get_ambiguity_score(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    ambiguity_score = sum(1 for word in words if len(nltk.corpus.wordnet.synsets(word)) > 1)\n",
    "    return ambiguity_score\n",
    "\n",
    "# Total number of positive, negative, and objective words in a sentence\n",
    "def count_words(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    pos_words = 0\n",
    "    neg_words = 0\n",
    "    obj_words = 0\n",
    "    for word in words:\n",
    "        sentiment = get_sentiment(word)\n",
    "        if sentiment > 0:\n",
    "            pos_words += 1\n",
    "        elif sentiment < 0:\n",
    "            neg_words += 1\n",
    "        else:\n",
    "            obj_words += 1\n",
    "    return pos_words, neg_words, obj_words\n",
    "\n",
    "# Main function\n",
    "def analyze_review(reviews):\n",
    "    sentiment_score = get_sentiment_score(reviews['Review_Text'])\n",
    "    intensity_score = get_intensity_score(reviews['Review_Text'])\n",
    "    ambiguity_score = get_ambiguity_score(reviews['Review_Text'])\n",
    "    pos_words, neg_words, obj_words = count_words(reviews['Review_Text'])\n",
    "    return sentiment_score, intensity_score, ambiguity_score, pos_words, neg_words, obj_words\n",
    "\n",
    "results_emotion = []\n",
    "\n",
    "for review in reviews:\n",
    "    sentiment_score, intensity_score, ambiguity_score, pos_words, neg_words, obj_words = analyze_review(review)\n",
    "    results_emotion.append({\n",
    "        'Sentiment_score': sentiment_score,\n",
    "        'Review_intensity': intensity_score,\n",
    "        'Review_ambiguity': ambiguity_score,\n",
    "        'Number_of_positive words:': pos_words,\n",
    "        'Number_of_negative words:': neg_words,\n",
    "        'Number_of_objective words:': obj_words,\n",
    "        })\n",
    "\n",
    "# Results dataframe\n",
    "df_emo_senti = pd.DataFrame(results_emotion)\n",
    "\n",
    "# Dataframe to Json file\n",
    "df_emo_senti.to_json('df_emo_senti.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion v2\n",
    "\n",
    "# Sentiment score per word function\n",
    "def calculate_sentiment_score(word):\n",
    "    synsets = list(swn.senti_synsets(word))\n",
    "    if not synsets:\n",
    "        return 0, 0\n",
    "    synset = synsets[0]\n",
    "    return synset.pos_score(), synset.neg_score()\n",
    "\n",
    "# Iterate through reviews and calculate\n",
    "results_emotion_2 = []\n",
    "for review in reviews:\n",
    "    text = review['Review_Text']\n",
    "    pos_125, neg_125, pos_25, neg_25, pos_375, neg_375, pos_5, neg_5, pos_625, neg_625, pos_75, neg_75, pos_875, neg_875, pos_1, neg_1 = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    for word in text.split():\n",
    "        pos_score, neg_score = calculate_sentiment_score(word)\n",
    "        if pos_score == 0 and neg_score == 0:\n",
    "            continue\n",
    "        if pos_score == 0.125:\n",
    "            pos_125 += 1\n",
    "        elif pos_score == 0.25:\n",
    "            pos_25 += 1\n",
    "        elif pos_score == 0.375:\n",
    "            pos_375 += 1\n",
    "        elif pos_score == 0.5:\n",
    "            pos_5 += 1\n",
    "        elif pos_score == 0.625:\n",
    "            pos_625 += 1\n",
    "        elif pos_score == 0.75:\n",
    "            pos_75 += 1\n",
    "        elif pos_score == 0.875:\n",
    "            pos_875 += 1\n",
    "        elif pos_score == 0.1:\n",
    "            pos_1 += 1\n",
    "        if neg_score == 0.125:\n",
    "            neg_125 += 1\n",
    "        elif neg_score == 0.25:\n",
    "            neg_25 += 1\n",
    "        elif neg_score == 0.375:\n",
    "            neg_375 += 1\n",
    "        elif neg_score == 0.5:\n",
    "            neg_5 += 1\n",
    "        elif neg_score == 0.625:\n",
    "            neg_625 += 1\n",
    "        elif neg_score == 0.75:\n",
    "            neg_75 += 1\n",
    "        elif neg_score == 0.875:\n",
    "            neg_875 += 1\n",
    "        elif neg_score == 1:\n",
    "            neg_1 += 1\n",
    "            \n",
    "    results_emotion_2.append({'pos_125': pos_125, 'neg_125': neg_125, 'pos_25': pos_25, 'neg_25': neg_25, 'pos_375': pos_375, 'neg_375': neg_375, 'pos_5': pos_5, 'neg_5': neg_5, 'pos_625': pos_625, 'neg_625': neg_625, 'pos_75': pos_75, 'neg_75': neg_75, 'pos_875': pos_875, 'neg_875': neg_875, 'pos_1': pos_1, 'neg_1': neg_1})\n",
    "\n",
    "# Results dataframe\n",
    "df_emo_2_senti = pd.DataFrame(results_emotion_2)\n",
    "\n",
    "# Dataframe to Json file\n",
    "df_emo_2_senti.to_json('df_emo_2_senti.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Sentiment_score</th>\n",
       "      <th>Review_intensity</th>\n",
       "      <th>Review_ambiguity</th>\n",
       "      <th>Number_of_positive words:</th>\n",
       "      <th>Number_of_negative words:</th>\n",
       "      <th>Number_of_objective words:</th>\n",
       "      <th>pos_125</th>\n",
       "      <th>neg_125</th>\n",
       "      <th>pos_25</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_5</th>\n",
       "      <th>neg_5</th>\n",
       "      <th>pos_625</th>\n",
       "      <th>neg_625</th>\n",
       "      <th>pos_75</th>\n",
       "      <th>neg_75</th>\n",
       "      <th>pos_875</th>\n",
       "      <th>neg_875</th>\n",
       "      <th>pos_1</th>\n",
       "      <th>neg_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label1__</td>\n",
       "      <td>1.375</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label1__</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.031250</td>\n",
       "      <td>52</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>68</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label1__</td>\n",
       "      <td>4.250</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label1__</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label1__</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Label  Sentiment_score  Review_intensity  Review_ambiguity  \\\n",
       "0  __label1__            1.375          0.796875                15   \n",
       "1  __label1__            1.000          1.031250                52   \n",
       "2  __label1__            4.250          2.375000                38   \n",
       "3  __label1__           -0.750          0.625000                30   \n",
       "4  __label1__            0.000          0.656250                44   \n",
       "\n",
       "   Number_of_positive words:  Number_of_negative words:  \\\n",
       "0                          3                          0   \n",
       "1                          9                          3   \n",
       "2                         10                          0   \n",
       "3                          5                          5   \n",
       "4                          2                          1   \n",
       "\n",
       "   Number_of_objective words:  pos_125  neg_125  pos_25  ...  pos_5  neg_5  \\\n",
       "0                          24        2        1       0  ...      0      0   \n",
       "1                          68        3        3       3  ...      0      0   \n",
       "2                          56        1        6       3  ...      3      0   \n",
       "3                          41        2        3       2  ...      0      0   \n",
       "4                          74        1        0       0  ...      1      0   \n",
       "\n",
       "   pos_625  neg_625  pos_75  neg_75  pos_875  neg_875  pos_1  neg_1  \n",
       "0        2        0       0       0        0        0      0      0  \n",
       "1        0        1       0       0        0        0      0      0  \n",
       "2        1        0       1       0        1        0      0      0  \n",
       "3        0        1       0       0        0        0      0      0  \n",
       "4        0        1       0       0        0        0      0      0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the working directory\n",
    "os.chdir('C:/Users/jesse/OneDrive/Documenten/Thesis/amazon_code/dataframes')\n",
    "\n",
    "# Load dataframes\n",
    "df_qua_senti = pd.read_json('df_qua_senti.json')\n",
    "df_com_senti = pd.read_json('df_com_senti.json')\n",
    "df_div_senti = pd.read_json('df_div_senti.json')\n",
    "df_emo_senti = pd.read_json('df_emo_senti.json')\n",
    "df_emo_2_senti = pd.read_json('df_emo_2_senti.json')\n",
    "\n",
    "# Final Dataframe\n",
    "senti_df = pd.concat([reviews_df['Label'], df_qua_senti, df_com_senti, df_div_senti, df_emo_senti, df_emo_2_senti], axis=1)\n",
    "\n",
    "# Dataframe to a Json file\n",
    "senti_df.to_json('senti_df.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label                 fake          real\n",
      "DOC_ID count  10500.000000  10500.000000\n",
      "       mean    5250.500000  15750.500000\n",
      "       std     3031.233247   3031.233247\n",
      "       min        1.000000  10501.000000\n",
      "       25%     2625.750000  13125.750000\n",
      "...                    ...           ...\n",
      "neg_1  min        0.000000      0.000000\n",
      "       25%        0.000000      0.000000\n",
      "       50%        0.000000      0.000000\n",
      "       75%        0.000000      0.000000\n",
      "       max        3.000000      2.000000\n",
      "\n",
      "[296 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Rename  labels\n",
    "senti_df['Label'] = senti_df['Label'].replace({'__label1__': 'fake', '__label2__': 'real'})\n",
    "\n",
    "# Group data by label and calculate statistics\n",
    "statistics_senti = senti_df.groupby('Label').describe().transpose()\n",
    "\n",
    "# Print statistics\n",
    "print(statistics_senti)\n",
    "\n",
    "statistics_senti.to_excel('statistics_senti.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Label  Number_of_words  Number_of_sentences  Number_of_caps  \\\n",
      "0      0         0.002822             0.010152        0.001144   \n",
      "1      0         0.020106             0.015228        0.002859   \n",
      "2      0         0.014109             0.030457        0.003431   \n",
      "3      0         0.010582             0.020305        0.004002   \n",
      "4      0         0.019400             0.015228        0.002859   \n",
      "\n",
      "   Number_of_punctuation  Number_of_nouns  Number_of_verbs  \\\n",
      "0               0.005357         0.005908         0.009615   \n",
      "1               0.012500         0.026588         0.025000   \n",
      "2               0.017857         0.013294         0.023077   \n",
      "3               0.008929         0.013294         0.019231   \n",
      "4               0.010714         0.023634         0.019231   \n",
      "\n",
      "   Number_of_adjectives  Number_of_adverbs  Average_word_length  ...  \\\n",
      "0              0.007663           0.009174             0.123009  ...   \n",
      "1              0.038314           0.022936             0.200208  ...   \n",
      "2              0.030651           0.022936             0.092044  ...   \n",
      "3              0.011494           0.018349             0.104161  ...   \n",
      "4              0.022989           0.022936             0.102568  ...   \n",
      "\n",
      "      pos_5  neg_5   pos_625   neg_625  pos_75  neg_75   pos_875  neg_875  \\\n",
      "0  0.000000    0.0  0.027027  0.000000   0.000     0.0  0.000000      0.0   \n",
      "1  0.000000    0.0  0.000000  0.028571   0.000     0.0  0.000000      0.0   \n",
      "2  0.055556    0.0  0.013514  0.042857   0.125     0.0  0.090909      0.0   \n",
      "3  0.000000    0.0  0.000000  0.014286   0.000     0.0  0.000000      0.0   \n",
      "4  0.000000    0.0  0.000000  0.014286   0.000     0.0  0.000000      0.0   \n",
      "\n",
      "   pos_1  neg_1  \n",
      "0    0.0    0.0  \n",
      "1    0.0    0.0  \n",
      "2    0.0    0.0  \n",
      "3    0.0    0.0  \n",
      "4    0.0    0.0  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Select the columns to normalize\n",
    "columns_to_normalize = ['Number_of_words', 'Number_of_sentences', 'Number_of_caps',\n",
    "       'Number_of_punctuation', 'Number_of_nouns', 'Number_of_verbs',\n",
    "       'Number_of_adjectives', 'Number_of_adverbs', 'Average_word_length',\n",
    "       'Average_sentence_length', 'Redundancy_score',\n",
    "       'Lexical_diversity', 'Review_intensity',\n",
    "       'Review_ambiguity', 'Number_of_positive words:',\n",
    "       'Number_of_negative words:', 'Number_of_objective words:', 'pos_125',\n",
    "       'neg_125', 'pos_25', 'neg_25', 'pos_375', 'neg_375', 'pos_5', 'neg_5',\n",
    "       'pos_625', 'neg_625', 'pos_75', 'neg_75', 'pos_875', 'neg_875', 'pos_1',\n",
    "       'neg_1']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# Normalize \n",
    "senti_df[columns_to_normalize] = scaler.fit_transform(senti_df[columns_to_normalize])\n",
    "\n",
    "# Dataframe to Json file\n",
    "senti_df.to_json('senti_df_norm.json', orient='records')\n",
    "print(senti_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
